# app/parser_youtube.py
import os
import aiohttp
import asyncio
import logging
from datetime import datetime
from tenacity import RetryError

from app.tagging import match_tags
from app.core.utils import api_retry

logger = logging.getLogger(__name__)

def _must(n: str) -> str:
    v = os.getenv(n)
    if not v:
        raise RuntimeError(f"ENV {n} is required")
    return v

SC_API_KEY = os.getenv("SCRAPECREATORS_KEY", "")
SC_BASE = "https://api.scrapecreators.com/v1/youtube"

# –º—è–≥–∫–∏–π –æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π –º–∞—Å—Å–æ–≤—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
MAX_RPM = int(os.getenv("SC_MAX_REQ_PER_MIN", "120"))
_MIN_INTERVAL = max(0.01, 60.0 / max(1, MAX_RPM))


# ----------------------------
# –ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã SC
# ----------------------------
@api_retry
async def _get_json(session: aiohttp.ClientSession, url: str, params: dict | None = None) -> dict:
    headers = {"x-api-key": SC_API_KEY, "accept": "application/json"}
    async with session.get(url, headers=headers, params=params, timeout=60) as resp:
        if resp.status != 200:
            txt = await resp.text()
            raise aiohttp.ClientResponseError(
                resp.request_info,
                resp.history,
                status=resp.status,
                message=f"SC {url} -> {resp.status}: {txt[:300]}",
            )
        return await resp.json()


async def fetch_shorts_simple(session, channel_id: str, amount: int = 20):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ YouTube Shorts —Å –∫–∞–Ω–∞–ª–∞ –ø–æ handle –∏–ª–∏ channelId.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ —Å–ª–æ–≤–∞—Ä–µ–π —Å {id, url, title, thumbnail, viewCountInt}
    """
    params = {"amount": str(amount)}

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º ‚Äî —ç—Ç–æ handle –∏–ª–∏ channelId
    if channel_id.startswith("UC"):
        params["channelId"] = channel_id
    else:
        params["handle"] = channel_id

    url = f"{SC_BASE}/channel/shorts/simple"

    try:
        data = await _get_json(session, url, params)
        logger.debug("üì¶ Shorts response example: %s", data[:1] if isinstance(data, list) else data)
        return data
    except Exception as e:
        logger.warning("‚ö†Ô∏è Fetch shorts failed after retries: %s", e)
        return []


async def fetch_video_details(session, video_url: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –≤–∏–¥–µ–æ (–ø—Ä–æ—Å–º–æ—Ç—Ä—ã, –ª–∞–π–∫–∏, –æ–ø–∏—Å–∞–Ω–∏–µ, –∫–∞–Ω–∞–ª –∏ —Ç.–¥.)
    """
    params = {"url": video_url}
    try:
        return await _get_json(session, f"{SC_BASE}/video", params)
    except Exception as e:
        logger.warning("‚ö†Ô∏è Fetch video details failed after retries: %s", e)
        return None


def _iso_from_text(date_text: str) -> tuple[int, int, datetime | None]:
    """
    –ü—Ä–æ–±—É–µ–º –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å publishDateText –∏–∑ ScrapeCreators –≤ UTC –¥–∞—Ç—É.
    –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ ‚Äî –≤–µ—Ä–Ω—ë–º (—Ç–µ–∫—É—â–∏–π –≥–æ–¥/–Ω–µ–¥–µ–ª—é, None).
    """
    try:
        # –ü—Ä–∏–º–µ—Ä: "Oct 21, 2025"
        dt = datetime.strptime(date_text, "%b %d, %Y")
    except Exception:
        try:
            # –ò–Ω–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç ‚Äú2025-10-21‚Äù –∏–ª–∏ –∏–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            dt = datetime.fromisoformat(date_text.replace("Z","").replace("T"," "))
        except Exception:
            dt = datetime.utcnow()
            return dt.isocalendar().year, dt.isocalendar().week, None
    iso = dt.isocalendar()
    return iso.year, iso.week, dt


# ---------------------------------------
# –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞
# ---------------------------------------
async def process_youtube_channel(
    session: aiohttp.ClientSession,
    conn,                       # asyncpg connection
    PG_SCHEMA: str,
    channel_id: str,
    amount: int,
    tags: list[dict],
    *,
    log_prefix: str = "YT",
    sleep_between: float = 0.2,
) -> dict:
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞:
      1) –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ shorts (id + url);
      2) –ø–æ –∫–∞–∂–¥–æ–º—É ‚Äî –¥–µ—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ;
      3) –ø–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫/–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ç–µ–≥–∏;
      4) upsert –≤ —Ç–∞–±–ª–∏—Ü—É video_stats;
      5) –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–Ω–∞–ª—É.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict:
      {
        'channel_id': ...,
        'total_shorts': N,
        'details_ok': K,
        'inserted': a,
        'updated': b,
        'skipped': c,
        'failed': f
      }
    """
    stats = {
        "channel_id": channel_id,
        "total_shorts": 0,
        "details_ok": 0,
        "inserted": 0,
        "updated": 0,
        "skipped": 0,
        "failed": 0,
    }

    logger.info("üéØ [%s] Start channel %s | amount=%s", log_prefix, channel_id, amount)
    shorts = await fetch_shorts_simple(session, channel_id, amount)
    stats["total_shorts"] = len(shorts)
    logger.info("üé¨ [%s] %s: fetched %d shorts", log_prefix, channel_id, stats["total_shorts"])

    if not shorts:
        logger.warning("üö´ [%s] %s: no shorts, skip channel", log_prefix, channel_id)
        return stats

    async with conn.transaction():
        for idx, s in enumerate(shorts, start=1):
            v_url = s.get("url")
            v_id = s.get("id")
            if not v_url:
                stats["skipped"] += 1
                logger.debug("‚Ü©Ô∏è [%s] %s: #%d skipped (no url)", log_prefix, channel_id, idx)
                continue

            try:
                details = await fetch_video_details(session, v_url)
                if not details:
                    stats["failed"] += 1
                    logger.warning("‚ùå [%s] %s: #%d details None (%s)", log_prefix, channel_id, idx, v_url)
                    await asyncio.sleep(sleep_between)
                    continue

                stats["details_ok"] += 1

                title = details.get("title") or ""
                descr = details.get("description") or ""
                text = f"{title}\n{descr}".strip()

                client_tag, company, product, matched_list = match_tags(text, tags)

                publish_text = details.get("publishDateText") or ""
                iso_year, week, dt = _iso_from_text(publish_text)
                publish_date = dt.date() if dt else datetime.utcnow().date()

                channel = details.get("channel") or {}
                account = channel.get("handle") or channel.get("title") or channel.get("id") or channel_id

                views = int(details.get("viewCountInt") or 0)
                likes = int(details.get("likeCountInt") or 0)
                comments = int(details.get("commentCountInt") or 0)

                # upsert
                res = await conn.execute(
                    f"""
                    INSERT INTO {PG_SCHEMA}.video_stats
                        (platform, account, video_id, video_url, publish_date,
                         iso_year, week, likes, views, comments, caption,
                         client_tag, company, product, created_at, updated_at)
                    VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,NOW(),NOW())
                    ON CONFLICT (video_url) DO UPDATE SET
                        account = EXCLUDED.account,
                        likes = EXCLUDED.likes,
                        views = EXCLUDED.views,
                        comments = EXCLUDED.comments,
                        caption = EXCLUDED.caption,
                        client_tag = EXCLUDED.client_tag,
                        company = EXCLUDED.company,
                        product = EXCLUDED.product,
                        updated_at = NOW();
                    """,
                    "youtube",
                    account,
                    v_id or details.get("id") or "",
                    details.get("url") or v_url,
                    publish_date,
                    iso_year,
                    week,
                    likes,
                    views,
                    comments,
                    text,
                    client_tag,
                    company,
                    product,
                )

                # asyncpg –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Ç–∏–ø–∞ "INSERT 0 1" –∏–ª–∏ "UPDATE 1"
                if res.startswith("INSERT"):
                    stats["inserted"] += 1
                    action = "INS"
                elif res.startswith("UPDATE"):
                    stats["updated"] += 1
                    action = "UPD"
                else:
                    stats["skipped"] += 1
                    action = "SKIP"

                logger.debug(
                    "‚úÖ [%s] %s: #%d %s id=%s views=%s likes=%s matched=%s",
                    log_prefix, channel_id, idx, action, (v_id or details.get("id")), views, likes, matched_list
                )

            except Exception as e:
                stats["failed"] += 1
                logger.exception("üí• [%s] %s: #%d fatal on %s | err=%s", log_prefix, channel_id, idx, v_url, e)

            await asyncio.sleep(sleep_between)

            # –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            if idx % 25 == 0 or idx == stats["total_shorts"]:
                logger.info(
                    "üìä [%s] %s: progress %d/%d | ins=%d upd=%d skip=%d fail=%d",
                    log_prefix, channel_id, idx, stats["total_shorts"],
                    stats["inserted"], stats["updated"], stats["skipped"], stats["failed"]
                )

    logger.info(
        "üèÅ [%s] %s: done | total=%d details_ok=%d ins=%d upd=%d skip=%d fail=%d",
        log_prefix, channel_id, stats["total_shorts"], stats["details_ok"],
        stats["inserted"], stats["updated"], stats["skipped"], stats["failed"]
    )
    return stats
